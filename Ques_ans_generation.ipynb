{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmYWbURSEfY5",
        "outputId": "1eb0dcf8-0ebe-4885-c479-821eaad6fbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m855.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "pMhwdvB8Eh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the key\n",
        "openai_api_key='your key'\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ],
      "metadata": {
        "id": "-X7zYz48EkF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMNHmD7FEo9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation Function"
      ],
      "metadata": {
        "id": "XJXNGfRoEsbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input num_of_ques any value if you want to generate questions such that it covers all the text\n",
        "\n",
        "#If a particular number of questions are desired then uncomment the code in the function\n",
        "\n",
        "def generate_ques(llm_model,text,num_of_ques):\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "  from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "  from langchain.llms import OpenAI\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "  from langchain import PromptTemplate, LLMChain\n",
        "  from langchain.prompts.chat import(\n",
        "      ChatPromptTemplate,\n",
        "      SystemMessagePromptTemplate,\n",
        "      AIMessagePromptTemplate,\n",
        "      HumanMessagePromptTemplate\n",
        "  )\n",
        "\n",
        "  from langchain.schema import(\n",
        "      AIMessage,\n",
        "      HumanMessage,\n",
        "      SystemMessage\n",
        "  )\n",
        "  chat=ChatOpenAI(temperature=0.5)\n",
        "  messages=[\n",
        "      #Uncomment the command below to generate a particular number of questions\n",
        "      #SystemMessage(content=\"Generate {0} of questions only from the text given by user\".format(num_of_ques)),\n",
        "    SystemMessage(content=\"Generate a particular number of questions from the text given by user such that the questions covers all the information in the text\"),\n",
        "    HumanMessage(content=text)\n",
        "  ]\n",
        "  resp= chat(messages)\n",
        "  resl=resp.content\n",
        "  return resl\n",
        "\n"
      ],
      "metadata": {
        "id": "YDl9XLdzEvXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KdxM-cbFtUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1oN7MPRFw2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Division of questions"
      ],
      "metadata": {
        "id": "s_DPDYlrFw9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_batches(ques, max_questions_per_batch):\n",
        "  import re\n",
        "  input_string=ques\n",
        "  #Splitting questions into a single element of a list\n",
        "  questions=re.split(r'\\n',input_string)\n",
        "\n",
        "  import math\n",
        "  # Grouping them in bacthes of desired size\n",
        "  num_batches = math.ceil(len(questions) / max_questions_per_batch)\n",
        "  batches = []\n",
        "\n",
        "  for i in range(num_batches):\n",
        "      start_idx = i * max_questions_per_batch\n",
        "      end_idx = (i + 1) * max_questions_per_batch\n",
        "      batch = questions[start_idx:end_idx]\n",
        "      batches.append(batch)\n",
        "\n",
        "  return batches\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JTz8hp-1F1C9",
        "outputId": "fd52cd5b-a339-49eb-fc89-fec2347a91e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  import re\\n  input_string=questions\\n  #Splitting questions into a single element of a list\\n  ques=re.split(r'\\n',input_string)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb4jo6lDGRQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Generation Function"
      ],
      "metadata": {
        "id": "IglfB2AUGReX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(ques,context):\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "  from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "  from langchain.llms import OpenAI\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "  from langchain import PromptTemplate, LLMChain\n",
        "  from langchain.prompts.chat import(\n",
        "      ChatPromptTemplate,\n",
        "      SystemMessagePromptTemplate,\n",
        "      AIMessagePromptTemplate,\n",
        "      HumanMessagePromptTemplate\n",
        "  )\n",
        "\n",
        "  from langchain.schema import(\n",
        "      AIMessage,\n",
        "      HumanMessage,\n",
        "      SystemMessage\n",
        "  )\n",
        "  chat=ChatOpenAI(temperature=0.5)\n",
        "  ans=[]\n",
        "  n=len(ques)\n",
        "  for i in range(n):\n",
        "    for q in ques[i]:\n",
        "      messages=[\n",
        "      SystemMessage(content=\"Answer within the scope of {0}\".format(context)),\n",
        "      HumanMessage(content=q)\n",
        "      ]\n",
        "      resp= chat(messages)\n",
        "      ans.append(resp.content)\n",
        "\n",
        "  return ans\n",
        "\n"
      ],
      "metadata": {
        "id": "39VhR2X5GPY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoYIUQJiGcpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing in JSON Format"
      ],
      "metadata": {
        "id": "DNqS1jDdGkF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to store the output in json format, it will give a txt file on being runned\n",
        "# Output file name , questions and answers list are passed as parameters\n",
        "\n",
        "\n",
        "def json_write_to_file(file_name, ques,ans):\n",
        "  n=len(ques)\n",
        "  l=[]\n",
        "  for i in range(n):\n",
        "    for x in ques[i]:\n",
        "      l.append(x)\n",
        "  file_name=file_name\n",
        "  import json\n",
        "  with open(file_name,'w') as file:\n",
        "    for x,y in zip(l,ans):\n",
        "      data={\"prompt\":x,\"completion\":y}\n",
        "      formatted_data=json.dumps(data)\n",
        "      file.write(formatted_data)\n",
        "      file.write('\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3yTyP64b2Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6g1NUxRjHLG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Illustrating with an example"
      ],
      "metadata": {
        "id": "NPvtKifzHLOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking an article for example\n",
        "\n",
        "article='Fuel is any substance used to produce heat and energy through a chemical or nuclear reaction. The energy is used by the conversion of a portion of the fuels mass. In India, we are suffering a severe fuel crises. In view of this, the Petroleum Conservation Research Association is encouraging fuel conservation. Our goal should be to reduce gasoline consumption progressively each year.'"
      ],
      "metadata": {
        "id": "BHYMJq_PIG5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a llm\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.llms import OpenAI\n",
        "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)\n",
        "\n",
        "llm = OpenAI(model_name='text-davinci-003',\n",
        "             temperature=0.9,\n",
        "             max_tokens = 256)\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import(\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "from langchain.schema import(\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "chat=ChatOpenAI(temperature=0.5)"
      ],
      "metadata": {
        "id": "OuX9LFyBIsbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating questions"
      ],
      "metadata": {
        "id": "lFLedP52MYQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating questions such that all text is covered\n",
        "ques= generate_ques(chat, article, 5)"
      ],
      "metadata": {
        "id": "9ZblrclkI1G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex1=ques\n",
        "ex1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "aJyqApmlJOVR",
        "outputId": "0bc55046-6364-48fb-d46b-9886ef1b14a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1. What is fuel and how is it used to produce heat and energy?\\n2. What is the role of the Petroleum Conservation Research Association in India?\\n3. Why is there a fuel crisis in India?\\n4. How does the conversion of a portion of the fuel's mass contribute to the production of energy?\\n5. What is the goal of the Petroleum Conservation Research Association in terms of gasoline consumption?\\n6. How does the association encourage fuel conservation?\\n7. What are the potential consequences of not reducing gasoline consumption in India?\\n8. Are there any alternative sources of energy being promoted by the association?\\n9. What are some measures individuals can take to conserve fuel?\\n10. How can reducing gasoline consumption help alleviate the fuel crisis in India?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJmT6y1wMnaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividing into batches"
      ],
      "metadata": {
        "id": "sIsOTzAjMno2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Diving into batches of 4 each\n",
        "ques=divide_batches(ex1,4)"
      ],
      "metadata": {
        "id": "GqqmbqOyJQb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6NiFb3gKDDh",
        "outputId": "66356061-78de-4aca-d01e-8c69461a4c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['1. What is fuel and how is it used to produce heat and energy?',\n",
              "  '2. What is the role of the Petroleum Conservation Research Association in India?',\n",
              "  '3. Why is there a fuel crisis in India?',\n",
              "  \"4. How does the conversion of a portion of the fuel's mass contribute to the production of energy?\"],\n",
              " ['5. What is the goal of the Petroleum Conservation Research Association in terms of gasoline consumption?',\n",
              "  '6. How does the association encourage fuel conservation?',\n",
              "  '7. What are the potential consequences of not reducing gasoline consumption in India?',\n",
              "  '8. Are there any alternative sources of energy being promoted by the association?'],\n",
              " ['9. What are some measures individuals can take to conserve fuel?',\n",
              "  '10. How can reducing gasoline consumption help alleviate the fuel crisis in India?']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(ques)"
      ],
      "metadata": {
        "id": "JdScVwfWNc1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oUCYcJM9NdbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answering the questions"
      ],
      "metadata": {
        "id": "ZYgmCsGeMkFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ans=answer(ques,article)"
      ],
      "metadata": {
        "id": "aMtMy8tdMlv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "id": "ypdsu2IwMwax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rZ7lhExRFfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing in json"
      ],
      "metadata": {
        "id": "2ynNZa7LO0U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"out.txt\"\n",
        "\n",
        "\n",
        "json_write_to_file(file_name, ques,ans)\n",
        "\n",
        "print(\"Output has been written to\", file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgpHndnUS-be",
        "outputId": "c525f5e4-939c-41bc-d552-fac36fd9b38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output has been written to out.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We get a file named out.txt and this can be directly used for fine-tuning"
      ],
      "metadata": {
        "id": "GObaWEPgVOlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJ_N4FUFexi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10JDzbbDeAU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}